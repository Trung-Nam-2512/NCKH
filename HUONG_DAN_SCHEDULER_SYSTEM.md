# H∆Ø·ªöNG D·∫™N H·ªÜ TH·ªêNG SCHEDULER V√Ä THU TH·∫¨P D·ªÆ LI·ªÜU T·ª∞ ƒê·ªòNG

## üìã T·ªîNG QUAN

H·ªá th·ªëng scheduler m·ªõi ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n thu th·∫≠p d·ªØ li·ªáu h√†ng ng√†y t·ª´ c√°c API ch·ªâ cung c·∫•p d·ªØ li·ªáu ng√†y hi·ªán t·∫°i. H·ªá th·ªëng ƒë·∫£m b·∫£o:

- ‚úÖ **Kh√¥ng m·∫•t d·ªØ li·ªáu**: Thu th·∫≠p v√† l∆∞u tr·ªØ d·ªØ li·ªáu l·ªãch s·ª≠ cho ph√¢n t√≠ch t·∫ßn su·∫•t
- ‚úÖ **Tr√°nh tr√πng l·∫∑p**: S·ª≠ d·ª•ng upsert operations thay v√¨ delete + insert
- ‚úÖ **T·ª± ƒë·ªông h√≥a**: Scheduler ch·∫°y ng·∫ßm thu th·∫≠p d·ªØ li·ªáu theo l·ªãch tr√¨nh
- ‚úÖ **Monitoring**: API endpoints v√† dashboard ƒë·ªÉ theo d√µi
- ‚úÖ **Recovery**: Error handling v√† retry logic

## üèóÔ∏è KI·∫æN TR√öC H·ªÜ TH·ªêNG

### 1. Components Ch√≠nh

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     SCHEDULER SYSTEM                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üïê SchedulerService                                           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ APScheduler (Advanced Python Scheduler)                   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Multiple daily collection times                           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Health monitoring                                         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Error recovery & retry logic                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üìä DailyDataCollector                                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ API data fetching (nokttv + kttv)                        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Data processing & validation                              ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Deduplication via upsert operations                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Historical data storage                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üåê SchedulerRouter (/scheduler/*)                             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Management API endpoints                                  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Manual collection triggers                                ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ System health monitoring                                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Logs and statistics                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. Database Collections

```
üìö MongoDB Collections:
‚îú‚îÄ‚îÄ üìñ historical_realtime_data     # D·ªØ li·ªáu l·ªãch s·ª≠ cho ph√¢n t√≠ch t·∫ßn su·∫•t
‚îú‚îÄ‚îÄ üìñ realtime_depth              # D·ªØ li·ªáu hi·ªán t·∫°i cho display (gi·ªØ nguy√™n)
‚îú‚îÄ‚îÄ üìñ data_collection_logs        # Logs c·ªßa collection process
‚îî‚îÄ‚îÄ üìñ historical_backup           # Backup d·ªØ li·ªáu ƒë·ªÉ recovery
```

## üöÄ TRI·ªÇN KHAI V√Ä CH·∫†Y H·ªÜ TH·ªêNG

### 1. C√†i ƒê·∫∑t Dependencies

```bash
# C√†i ƒë·∫∑t th√™m packages cho scheduler
cd backend
pip install -r requirements_scheduler.txt

# Ho·∫∑c c√†i t·ª´ng package:
pip install apscheduler==3.10.4 motor pymongo httpx pydantic
```

### 2. C·∫•u H√¨nh Environment Variables

ƒê·∫£m b·∫£o file `.env` c√≥ ƒë·∫ßy ƒë·ªß:

```env
# MongoDB
MONGO_URI=mongodb+srv://username:password@cluster.mongodb.net/database_name

# API Configuration
STATIONS_API_BASE_URL_NOKTTV=https://api.nokttv.example.com/stations
STATS_API_BASE_URL_NOKTTV=https://api.nokttv.example.com/stats
STATIONS_API_BASE_URL_KTTV=https://api.kttv.example.com/stations
STATS_API_BASE_URL_KTTV=https://api.kttv.example.com/stats
API_KEY=your_api_key_here

# Database name
DATABASE_NAME=your_database_name
```

### 3. Test H·ªá Th·ªëng

```bash
# Test comprehensive t·∫•t c·∫£ components
cd backend
python test_daily_collector.py

# Test manual collection
python run_scheduler.py --test

# Test specific date collection
python run_scheduler.py --manual-collect --date 2024-01-15
```

### 4. Ch·∫°y Scheduler

#### Option 1: Foreground (Development)
```bash
cd backend
python run_scheduler.py
```

#### Option 2: Background (Production)
```bash
# Ch·∫°y trong background
cd backend
nohup python run_scheduler.py > scheduler.log 2>&1 &

# Ho·∫∑c s·ª≠ d·ª•ng screen/tmux
screen -S scheduler
python run_scheduler.py
# Ctrl+A, D ƒë·ªÉ detach
```

#### Option 3: Systemd Service (Linux Production)
```bash
# T·∫°o systemd service file
sudo python run_scheduler.py --create-service

# Enable v√† start service
sudo systemctl daemon-reload
sudo systemctl enable nckh-scheduler
sudo systemctl start nckh-scheduler

# Check status
sudo systemctl status nckh-scheduler
sudo journalctl -u nckh-scheduler -f
```

## üìä MONITORING V√Ä QU·∫¢N L√ù

### 1. API Endpoints

Sau khi kh·ªüi ƒë·ªông FastAPI server v·ªõi scheduler router:

#### Ki·ªÉm Tra Tr·∫°ng Th√°i
```bash
curl http://localhost:8000/scheduler/status
```

#### Manual Collection
```bash
# Trigger manual collection cho h√¥m nay
curl -X POST http://localhost:8000/scheduler/manual-collect

# Collection cho ng√†y c·ª• th·ªÉ
curl -X POST http://localhost:8000/scheduler/manual-collect \
  -H "Content-Type: application/json" \
  -d '{"target_date": "2024-01-15"}'
```

#### Xem Logs
```bash
curl http://localhost:8000/scheduler/logs?limit=20&days=7
```

#### Health Check
```bash
curl http://localhost:8000/scheduler/health
```

#### Xem Job Schedule
```bash
curl http://localhost:8000/scheduler/jobs
```

### 2. Log Files

```bash
# Scheduler logs
tail -f scheduler_production.log

# Collection logs
tail -f daily_collector.log

# FastAPI logs
tail -f uvicorn.log  # N·∫øu c√≥
```

## ‚è∞ L·ªäCH TR√åNH COLLECTION

### Default Schedule

Scheduler ƒë∆∞·ª£c c·∫•u h√¨nh ch·∫°y **3 l·∫ßn m·ªói ng√†y** ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ d·ªØ li·ªáu:

- üåÖ **06:30 UTC** (14:30 VN time) - S√°ng
- üåû **14:30 UTC** (22:30 VN time) - Chi·ªÅu  
- üåô **22:30 UTC** (06:30+1 VN time) - T·ªëi

### T·∫°i Sao 3 L·∫ßn?

1. **API Reliability**: APIs c√≥ th·ªÉ t·∫°m th·ªùi unavailable
2. **Data Availability**: D·ªØ li·ªáu c√≥ th·ªÉ ƒë∆∞·ª£c c·∫≠p nh·∫≠t v√†o c√°c th·ªùi ƒëi·ªÉm kh√°c nhau trong ng√†y
3. **Fault Tolerance**: N·∫øu 1-2 l·∫ßn failed, v·∫´n c√≥ c∆° h·ªôi thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu

### Customization

C√≥ th·ªÉ thay ƒë·ªïi trong `SchedulerService.__init__()`:

```python
'collection_times': [
    {'hour': 6, 'minute': 30},    # 6:30 AM UTC
    {'hour': 14, 'minute': 30},   # 2:30 PM UTC  
    {'hour': 22, 'minute': 30}    # 10:30 PM UTC
]
```

## üîß TROUBLESHOOTING

### 1. Common Issues

#### Database Connection Failed
```bash
# Check MongoDB URI
echo $MONGO_URI

# Test connection manually
python -c "
from motor.motor_asyncio import AsyncIOMotorClient
import asyncio
async def test():
    client = AsyncIOMotorClient('your_uri')
    await client.admin.command('ismaster')
    print('‚úÖ Connection successful')
asyncio.run(test())
"
```

#### API Authentication Failed
```bash
# Check API key
echo $API_KEY

# Test API manually
curl -H "x-api-key: $API_KEY" https://your-api-endpoint.com/stations
```

#### No Data Collected
```bash
# Check API responses
python run_scheduler.py --test --log-level DEBUG

# Check database
mongo "your_connection_string" --eval "
db.historical_realtime_data.find().limit(5).pretty()
"
```

### 2. Performance Issues

#### High Memory Usage
- Gi·∫£m `batch_size` trong collection process
- Implement data streaming for large datasets
- Add memory monitoring

#### Slow Collection
- Check network latency to APIs
- Implement parallel API calls
- Add connection pooling

### 3. Data Issues

#### Duplicate Records
- Verify unique indexes created
- Check upsert logic
- Review collection logs

#### Missing Data
- Check API availability during collection times
- Review error logs
- Implement backfill script

## üìà DATA FLOW CHO FREQUENCY ANALYSIS

### 1. Data Storage Strategy

```
üîÑ Daily Collection Flow:
‚îú‚îÄ‚îÄ üì• Fetch t·ª´ APIs (nokttv + kttv)
‚îú‚îÄ‚îÄ üîÑ Process & validate data  
‚îú‚îÄ‚îÄ üíæ Upsert v√†o historical_realtime_data (with deduplication)
‚îú‚îÄ‚îÄ üîÑ Update realtime_depth (for current display)
‚îî‚îÄ‚îÄ üìù Log collection results
```

### 2. Historical Data Structure

```json
{
  "_id": "ObjectId(...)",
  "station_id": "056882",
  "code": "056882", 
  "name": "Tr·∫°m Th·ªß ƒê·ª©c",
  "latitude": 10.8721,
  "longitude": 106.7674,
  "api_type": "nokttv",
  "time_point": "2024-01-15T14:30:00Z",
  "depth": 1.25,
  "collection_date": "2024-01-15",
  "created_at": "2024-01-15T22:30:45Z"
}
```

### 3. Frequency Analysis Integration

```javascript
// Frontend c√≥ th·ªÉ query historical data
const response = await fetch('/api/data/historical', {
  method: 'POST',
  body: JSON.stringify({
    station_id: '056882',
    start_date: '2023-01-01',
    end_date: '2024-01-15',
    aggregation: 'daily_max'  // L·∫•y max depth per day
  })
});
```

## üõ°Ô∏è BACKUP V√Ä RECOVERY

### 1. Automatic Backups

H·ªá th·ªëng t·ª± ƒë·ªông backup d·ªØ li·ªáu tr∆∞·ªõc khi collection:

- Backup stored trong `historical_backup` collection
- Retention: 7 days (configurable)
- Automatic cleanup via scheduler

### 2. Manual Backup

```bash
# Export collection
mongoexport --uri="your_connection_string" \
  --collection=historical_realtime_data \
  --out=backup_$(date +%Y%m%d).json

# Import back
mongoimport --uri="your_connection_string" \
  --collection=historical_realtime_data \
  --file=backup_20240115.json
```

### 3. Data Recovery

```python
# Script ƒë·ªÉ recover t·ª´ backup
from app.services.daily_data_collector import DailyDataCollector
import asyncio

async def recover_data(backup_date):
    collector = DailyDataCollector()
    await collector.initialize_database()
    
    # Find backup
    backup = await collector.db.historical_backup.find_one({
        'target_date': backup_date
    })
    
    if backup:
        # Restore data
        await collector.db.historical_realtime_data.insert_many(backup['data'])
        print(f"‚úÖ Recovered {len(backup['data'])} records")

# Usage
asyncio.run(recover_data('2024-01-15'))
```

## üîÆ FUTURE ENHANCEMENTS

### 1. Advanced Features

- **Real-time Alerting**: Email/SMS alerts khi collection failed
- **Data Validation**: Advanced QC checks cho d·ªØ li·ªáu
- **Predictive Collection**: Machine learning ƒë·ªÉ predict optimal collection times
- **Multi-region**: Support multiple geographic regions
- **API Versioning**: Handle API changes gracefully

### 2. Performance Optimizations

- **Caching Layer**: Redis cache cho frequently accessed data
- **Data Compression**: Compress historical data
- **Partitioning**: Time-based partitioning for large datasets
- **Indexing**: Advanced indexing strategies

### 3. Integration Improvements

- **Grafana Dashboard**: Visual monitoring dashboard
- **Prometheus Metrics**: Detailed metrics collection
- **Docker Support**: Containerized deployment
- **Kubernetes**: Scalable container orchestration

## üìû SUPPORT

### 1. Logs Location

- **Scheduler**: `scheduler_production.log`
- **Collection**: `daily_collector.log`
- **FastAPI**: Check uvicorn logs

### 2. Debug Mode

```bash
python run_scheduler.py --test --log-level DEBUG
```

### 3. Health Monitoring

Th∆∞·ªùng xuy√™n check health endpoint:

```bash
curl http://localhost:8000/scheduler/health | jq
```

---

**üéØ K·∫øt lu·∫≠n**: H·ªá th·ªëng scheduler n√†y gi·∫£i quy·∫øt ho√†n to√†n b√†i to√°n thu th·∫≠p d·ªØ li·ªáu h√†ng ng√†y t·ª´ APIs ch·ªâ cung c·∫•p d·ªØ li·ªáu ng√†y hi·ªán t·∫°i, ƒë·∫£m b·∫£o c√≥ ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ cho ph√¢n t√≠ch t·∫ßn su·∫•t m√† kh√¥ng g√¢y tr√πng l·∫∑p hay m·∫•t d·ªØ li·ªáu.