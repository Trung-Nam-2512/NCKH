#!/usr/bin/env python3
"""
DAILY DATA COLLECTOR SERVICE - Script ch·∫°y ng·∫ßm thu th·∫≠p d·ªØ li·ªáu API h√†ng ng√†y

Ch·ª©c nƒÉng ch√≠nh:
- Thu th·∫≠p d·ªØ li·ªáu t·ª´ c√°c API theo ng√†y (v√¨ API ch·ªâ cung c·∫•p d·ªØ li·ªáu ng√†y hi·ªán t·∫°i)
- Tr√°nh ghi ƒë√® d·ªØ li·ªáu v√† tr√πng l·∫∑p b·∫£n ghi
- L∆∞u tr·ªØ d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ ph·ª•c v·ª• ph√¢n t√≠ch t·∫ßn su·∫•t
- T·ª± ƒë·ªông ch·∫°y h√†ng ng√†y qua scheduler
- Log chi ti·∫øt ƒë·ªÉ monitoring v√† debugging

Thi·∫øt k·∫ø:
- S·ª≠ d·ª•ng upsert operations thay v√¨ delete_all + insert
- T·∫°o unique index ƒë·ªÉ tr√°nh duplicate
- Backup d·ªØ li·ªáu tr∆∞·ªõc khi c·∫≠p nh·∫≠t  
- Graceful error handling v√† recovery
- Comprehensive logging cho audit trail
"""

import asyncio
import httpx
import logging
import sys
import os
from datetime import datetime, timedelta, date
from typing import List, Dict, Optional
from motor.motor_asyncio import AsyncIOMotorClient
from pymongo import UpdateOne
from pymongo.errors import BulkWriteError
import json

# Add parent directory to path ƒë·ªÉ import config
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from config import *

# C·∫•u h√¨nh logging chi ti·∫øt v·ªõi timestamp v√† level
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',
    handlers=[
        logging.FileHandler('daily_collector.log'),
        logging.StreamHandler()
    ]
)

class DailyDataCollector:
    """
    SERVICE THU TH·∫¨P D·ªÆ LI·ªÜU H√ÄNG NG√ÄY
    
    Thi·∫øt k·∫ø ƒë·ªÉ tr√°nh c√°c v·∫•n ƒë·ªÅ c·ªßa h·ªá th·ªëng hi·ªán t·∫°i:
    1. Kh√¥ng x√≥a to√†n b·ªô d·ªØ li·ªáu (delete_many)
    2. S·ª≠ d·ª•ng upsert ƒë·ªÉ tr√°nh duplicate
    3. Gi·ªØ l·∫°i d·ªØ li·ªáu l·ªãch s·ª≠ cho ph√¢n t√≠ch t·∫ßn su·∫•t
    4. Ki·ªÉm tra data integrity tr∆∞·ªõc khi commit
    """
    
    def __init__(self):
        """Kh·ªüi t·∫°o collector v·ªõi c·∫•u h√¨nh an to√†n"""
        self.mongo_uri = MONGODB_URI
        self.client = None
        self.db = None
        
        # C·∫•u h√¨nh API endpoints
        self.api_configs = {
            'nokttv': {
                'stations_url': STATIONS_API_BASE_URL_NOKTTV,
                'stats_url': STATS_API_BASE_URL_NOKTTV,
                'api_key': API_KEY,
                'type': 'nokttv'
            },
            'kttv': {
                'stations_url': STATIONS_API_BASE_URL_KTTV,
                'stats_url': STATS_API_BASE_URL_KTTV,
                'api_key': None,
                'type': 'kttv'
            }
        }
        
        # Cache cho station mapping
        self.station_mapping = {}
        
        # Collection names
        self.collections = {
            'historical_data': 'historical_realtime_data',  # D·ªØ li·ªáu l·ªãch s·ª≠ cho ph√¢n t√≠ch t·∫ßn su·∫•t
            'current_data': 'realtime_depth',              # D·ªØ li·ªáu hi·ªán t·∫°i cho real-time display
            'collection_logs': 'data_collection_logs',     # Logs cho audit trail
            'backup': 'historical_backup'                  # Backup tr∆∞·ªõc khi update
        }

    async def initialize_database(self) -> bool:
        """
        Kh·ªüi t·∫°o k·∫øt n·ªëi database v√† t·∫°o indexes c·∫ßn thi·∫øt
        
        Returns:
            bool: True n·∫øu k·∫øt n·ªëi th√†nh c√¥ng
        """
        try:
            if not self.mongo_uri:
                logging.error("‚ùå No MongoDB URI provided")
                return False
                
            self.client = AsyncIOMotorClient(self.mongo_uri)
            self.db = self.client[DATABASE_NAME]
            
            # Test connection
            await self.client.admin.command('ping')
            
            # T·∫°o unique indexes ƒë·ªÉ tr√°nh duplicate data
            await self._create_indexes()
            
            logging.info("‚úÖ Database initialized successfully")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Database initialization failed: {e}")
            self.client = None
            self.db = None
            return False
    
    async def _create_indexes(self):
        """T·∫°o c√°c indexes c·∫ßn thi·∫øt ƒë·ªÉ tr√°nh duplicate v√† t·ªëi ∆∞u query"""
        try:
            # Index cho historical_data: unique combination ƒë·ªÉ tr√°nh duplicate
            await self.db[self.collections['historical_data']].create_index([
                ('station_id', 1),
                ('time_point', 1),
                ('api_type', 1)
            ], unique=True, background=True)
            
            # Index cho current_data (gi·ªØ nguy√™n structure hi·ªán t·∫°i)
            await self.db[self.collections['current_data']].create_index([
                ('station_id', 1),
                ('time_point', -1)
            ], background=True)
            
            # Index cho logs
            await self.db[self.collections['collection_logs']].create_index([
                ('collection_date', -1),
                ('status', 1)
            ], background=True)
            
            logging.info("‚úÖ Database indexes created successfully")
            
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Index creation warning (may already exist): {e}")

    async def fetch_stations(self, api_type: str) -> List[Dict]:
        """
        Fetch stations t·ª´ API v·ªõi proper error handling
        
        Args:
            api_type: 'nokttv' ho·∫∑c 'kttv'
            
        Returns:
            List[Dict]: Danh s√°ch stations
        """
        try:
            config = self.api_configs[api_type]
            url = config['stations_url']
            headers = {}
            
            if api_type == 'nokttv' and config['api_key']:
                headers['x-api-key'] = config['api_key']
            
            logging.info(f"üîç Fetching {api_type.upper()} stations from {url}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Handle different response formats
                    stations = []
                    if isinstance(data, list):
                        stations = data
                    elif isinstance(data, dict):
                        stations = data.get('data', data.get('stations', data.get('results', [])))
                    
                    if not isinstance(stations, list):
                        stations = []
                    
                    logging.info(f"‚úÖ Fetched {len(stations)} {api_type.upper()} stations")
                    return stations
                    
                else:
                    logging.warning(f"‚ö†Ô∏è {api_type.upper()} stations API failed: {response.status_code}")
                    return []
                    
        except Exception as e:
            logging.error(f"‚ùå Error fetching {api_type} stations: {e}")
            return []

    async def fetch_daily_stats(self, api_type: str) -> List[Dict]:
        """
        Fetch daily stats t·ª´ API
        
        Args:
            api_type: 'nokttv' ho·∫∑c 'kttv'
            
        Returns:
            List[Dict]: D·ªØ li·ªáu stats c·ªßa ng√†y hi·ªán t·∫°i
        """
        try:
            config = self.api_configs[api_type]
            url = config['stats_url']
            headers = {}
            
            if api_type == 'nokttv' and config['api_key']:
                headers['x-api-key'] = config['api_key']
            
            logging.info(f"üì• Fetching {api_type.upper()} daily stats from {url}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Handle different response formats
                    stats = []
                    if isinstance(data, list):
                        stats = data
                    elif isinstance(data, dict):
                        stats = data.get('data', data.get('stats', data.get('results', [])))
                    
                    if not isinstance(stats, list):
                        stats = []
                    
                    logging.info(f"‚úÖ Fetched {len(stats)} {api_type.upper()} daily stats")
                    return stats
                    
                else:
                    logging.warning(f"‚ö†Ô∏è {api_type.upper()} stats API failed: {response.status_code}")
                    return []
                    
        except Exception as e:
            logging.error(f"‚ùå Error fetching {api_type} daily stats: {e}")
            return []

    def create_station_mapping(self, stations: List[Dict], api_type: str):
        """
        T·∫°o mapping gi·ªØa station ID v√† th√¥ng tin station
        
        Args:
            stations: Danh s√°ch stations t·ª´ API
            api_type: Type c·ªßa API
        """
        mapped_count = 0
        
        for station in stations:
            # Handle different station ID key names
            station_id = None
            station_code = None
            
            for id_key in ['code', 'uuid', 'station_id', 'id', 'stationId']:
                if id_key in station and station[id_key]:
                    station_id = str(station[id_key])
                    station_code = station.get('code', station_id)  # Prefer 'code' for station_code
                    break
            
            # Extract coordinates v√† name
            lat = station.get('latitude', station.get('lat', None))
            lon = station.get('longitude', station.get('lon', station.get('lng', None)))
            name = station.get('name', station.get('station_name', f'Tr·∫°m {station_code}' if station_code else 'Unknown'))
            
            if station_id and lat is not None and lon is not None:
                mapping_entry = {
                    'code': station_code,
                    'name': name,
                    'latitude': float(lat),
                    'longitude': float(lon),
                    'api_type': api_type,
                    'last_updated': datetime.utcnow()
                }
                
                self.station_mapping[station_id] = mapping_entry
                mapped_count += 1
        
        logging.info(f"‚úÖ Mapped {mapped_count}/{len(stations)} {api_type.upper()} stations")

    async def process_daily_data(self, stats: List[Dict], api_type: str, target_date: date) -> List[Dict]:
        """
        X·ª≠ l√Ω d·ªØ li·ªáu daily v√† chu·∫©n b·ªã cho database update
        
        Args:
            stats: Raw data t·ª´ API
            api_type: Type c·ªßa API
            target_date: Ng√†y thu th·∫≠p d·ªØ li·ªáu
            
        Returns:
            List[Dict]: Documents ready ƒë·ªÉ insert v√†o database
        """
        documents = []
        processed_count = 0
        skipped_count = 0
        
        for stat in stats:
            # Extract station ID
            station_id = None
            for id_key in ['station_id', 'code', 'uuid', 'stationId', 'id']:
                if id_key in stat and stat[id_key]:
                    station_id = str(stat[id_key])
                    break
            
            if not station_id:
                skipped_count += 1
                continue
            
            # Check station mapping
            station_info = self.station_mapping.get(station_id)
            if not station_info:
                skipped_count += 1
                continue
            
            # Process measurements
            values = stat.get('value', stat.get('values', stat.get('data', [])))
            if not isinstance(values, list):
                values = [values] if values else []
            
            for value in values:
                try:
                    # Parse timestamp
                    time_point_str = value.get('time_point', value.get('timestamp', value.get('time', '')))
                    depth = value.get('depth', value.get('water_level', value.get('level', 0)))
                    
                    if time_point_str and depth is not None:
                        time_point = await self._parse_timestamp(time_point_str)
                        
                        if time_point and time_point.date() == target_date:
                            document = {
                                'station_id': station_id,
                                'code': station_info['code'],
                                'name': station_info['name'],
                                'latitude': station_info['latitude'],
                                'longitude': station_info['longitude'],
                                'api_type': api_type,
                                'time_point': time_point,
                                'depth': float(depth),
                                'collection_date': target_date,
                                'created_at': datetime.utcnow()
                            }
                            documents.append(document)
                            processed_count += 1
                            
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è Error processing measurement: {e}")
                    continue
        
        logging.info(f"üìä Processed {processed_count} measurements, skipped {skipped_count} from {api_type.upper()}")
        return documents

    async def _parse_timestamp(self, timestamp_str: str) -> Optional[datetime]:
        """
        Parse timestamp string v·ªõi multiple format support
        
        Args:
            timestamp_str: String timestamp t·ª´ API
            
        Returns:
            datetime object ho·∫∑c None n·∫øu parsing failed
        """
        try:
            if 'T' in str(timestamp_str):
                return datetime.fromisoformat(str(timestamp_str).replace('Z', '+00:00'))
            else:
                # Try multiple formats
                formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%Y/%m/%d %H:%M:%S']
                for fmt in formats:
                    try:
                        return datetime.strptime(str(timestamp_str), fmt)
                    except ValueError:
                        continue
        except Exception:
            pass
        
        return None

    async def backup_existing_data(self, target_date: date) -> bool:
        """
        Backup d·ªØ li·ªáu hi·ªán c√≥ tr∆∞·ªõc khi update (safety measure)
        
        Args:
            target_date: Ng√†y c·∫ßn backup
            
        Returns:
            bool: True n·∫øu backup th√†nh c√¥ng
        """
        try:
            # Query d·ªØ li·ªáu c·ªßa ng√†y target
            existing_data = await self.db[self.collections['historical_data']].find({
                'collection_date': target_date
            }).to_list(None)
            
            if existing_data:
                # Create backup document
                backup_doc = {
                    'backup_date': datetime.utcnow(),
                    'target_date': target_date,
                    'data_count': len(existing_data),
                    'data': existing_data
                }
                
                await self.db[self.collections['backup']].insert_one(backup_doc)
                logging.info(f"‚úÖ Backed up {len(existing_data)} records for {target_date}")
            
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Backup failed: {e}")
            return False

    async def upsert_historical_data(self, documents: List[Dict]) -> bool:
        """
        Upsert d·ªØ li·ªáu v√†o historical collection v·ªõi deduplication
        
        Args:
            documents: Danh s√°ch documents ƒë·ªÉ upsert
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        if not documents:
            logging.warning("‚ö†Ô∏è No documents to upsert")
            return True
        
        try:
            # Prepare upsert operations
            operations = []
            for doc in documents:
                filter_query = {
                    'station_id': doc['station_id'],
                    'time_point': doc['time_point'],
                    'api_type': doc['api_type']
                }
                
                update_doc = {'$set': doc}
                operations.append(UpdateOne(filter_query, update_doc, upsert=True))
            
            # Execute bulk upsert
            result = await self.db[self.collections['historical_data']].bulk_write(
                operations, 
                ordered=False
            )
            
            logging.info(f"‚úÖ Upserted: {result.upserted_count} new, {result.modified_count} updated")
            return True
            
        except BulkWriteError as e:
            logging.error(f"‚ùå Bulk write error: {e.details}")
            return False
        except Exception as e:
            logging.error(f"‚ùå Upsert failed: {e}")
            return False

    async def update_current_data(self, documents: List[Dict]) -> bool:
        """
        Update current data collection (gi·ªØ nguy√™n behavior hi·ªán t·∫°i cho real-time display)
        
        Args:
            documents: Documents ƒë·ªÉ update
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        if not documents:
            return True
        
        try:
            # Clear v√† insert (gi·ªØ nguy√™n logic hi·ªán t·∫°i)
            await self.db[self.collections['current_data']].delete_many({})
            
            # Insert new data
            batch_size = 1000
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i + batch_size]
                await self.db[self.collections['current_data']].insert_many(batch)
            
            logging.info(f"‚úÖ Updated current data: {len(documents)} records")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Current data update failed: {e}")
            return False

    async def log_collection_result(self, target_date: date, status: str, details: Dict):
        """
        Ghi log k·∫øt qu·∫£ collection ƒë·ªÉ audit v√† monitoring
        
        Args:
            target_date: Ng√†y collection
            status: 'success', 'partial', 'failed'
            details: Chi ti·∫øt v·ªÅ k·∫øt qu·∫£
        """
        try:
            log_doc = {
                'collection_date': target_date.isoformat() if isinstance(target_date, date) else target_date,
                'execution_time': datetime.utcnow(),
                'status': status,
                'details': details,
                'api_responses': {
                    api_type: details.get(f'{api_type}_count', 0) 
                    for api_type in ['nokttv', 'kttv']
                },
                'total_records': details.get('total_records', 0)
            }
            
            await self.db[self.collections['collection_logs']].insert_one(log_doc)
            logging.info(f"üìù Logged collection result: {status}")
            
        except Exception as e:
            logging.error(f"‚ùå Logging failed: {e}")

    async def run_daily_collection(self, target_date: Optional[date] = None) -> bool:
        """
        Main method ƒë·ªÉ ch·∫°y daily collection
        
        Args:
            target_date: Ng√†y c·∫ßn collect (default: h√¥m nay)
            
        Returns:
            bool: True n·∫øu collection th√†nh c√¥ng
        """
        if target_date is None:
            target_date = date.today()
        
        start_time = datetime.utcnow()
        logging.info(f"üöÄ Starting daily collection for {target_date}")
        
        try:
            # Initialize database
            if not await self.initialize_database():
                return False
            
            # Backup existing data
            await self.backup_existing_data(target_date)
            
            all_documents = []
            collection_details = {'total_records': 0}
            
            # Process both API types
            for api_type in ['nokttv', 'kttv']:
                try:
                    logging.info(f"üì° Processing {api_type.upper()} API...")
                    
                    # Fetch stations v√† t·∫°o mapping
                    stations = await self.fetch_stations(api_type)
                    if stations:
                        self.create_station_mapping(stations, api_type)
                    
                    # Fetch daily stats
                    stats = await self.fetch_daily_stats(api_type)
                    if stats:
                        documents = await self.process_daily_data(stats, api_type, target_date)
                        all_documents.extend(documents)
                        collection_details[f'{api_type}_count'] = len(documents)
                    else:
                        collection_details[f'{api_type}_count'] = 0
                        
                except Exception as e:
                    logging.error(f"‚ùå Error processing {api_type}: {e}")
                    collection_details[f'{api_type}_error'] = str(e)
            
            # Update databases
            if all_documents:
                collection_details['total_records'] = len(all_documents)
                
                # Update historical data (v·ªõi deduplication)
                historical_success = await self.upsert_historical_data(all_documents)
                
                # Update current data (cho real-time display)
                current_success = await self.update_current_data(all_documents)
                
                if historical_success and current_success:
                    status = 'success'
                elif historical_success or current_success:
                    status = 'partial'
                else:
                    status = 'failed'
            else:
                status = 'failed'
                collection_details['error'] = 'No data collected from any API'
            
            # Log k·∫øt qu·∫£
            collection_details['execution_time'] = (datetime.utcnow() - start_time).total_seconds()
            await self.log_collection_result(target_date, status, collection_details)
            
            logging.info(f"‚úÖ Daily collection completed: {status} - {len(all_documents)} records")
            return status in ['success', 'partial']
            
        except Exception as e:
            logging.error(f"‚ùå Daily collection failed: {e}")
            await self.log_collection_result(target_date, 'failed', {'error': str(e)})
            return False
            
        finally:
            if hasattr(self, 'client') and self.client:
                try:
                    await self.client.close()
                except Exception:
                    pass  # Ignore close errors
                finally:
                    self.client = None

    async def cleanup_old_logs(self, days_to_keep: int = 30):
        """
        Cleanup logs c≈© ƒë·ªÉ tr√°nh database bloat
        
        Args:
            days_to_keep: S·ªë ng√†y logs c·∫ßn gi·ªØ
        """
        try:
            cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
            
            result = await self.db[self.collections['collection_logs']].delete_many({
                'execution_time': {'$lt': cutoff_date}
            })
            
            logging.info(f"üßπ Cleaned up {result.deleted_count} old log entries")
            
        except Exception as e:
            logging.error(f"‚ùå Log cleanup failed: {e}")

if __name__ == "__main__":
    async def main():
        """Main entry point cho script ch·∫°y ƒë·ªôc l·∫≠p"""
        collector = DailyDataCollector()
        
        # Option to collect cho ng√†y kh√°c (for testing/backfill)
        target_date = None
        if len(sys.argv) > 1:
            try:
                target_date = datetime.strptime(sys.argv[1], '%Y-%m-%d').date()
                logging.info(f"üéØ Collecting data for specified date: {target_date}")
            except ValueError:
                logging.error("‚ùå Invalid date format. Use YYYY-MM-DD")
                sys.exit(1)
        
        success = await collector.run_daily_collection(target_date)
        
        if success:
            logging.info("‚úÖ Script completed successfully")
            sys.exit(0)
        else:
            logging.error("‚ùå Script completed with errors")
            sys.exit(1)
    
    asyncio.run(main())