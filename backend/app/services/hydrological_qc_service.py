"""
Advanced Hydrological Quality Control Service
Implements professional-grade QC procedures following WMO-No.168 and ISO 14688 standards
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Optional
from scipy import stats
from datetime import datetime, timedelta
from dataclasses import dataclass

@dataclass
class QCResult:
    """Data class for QC results"""
    flag: str  # 'good', 'suspect', 'bad', 'missing'
    value: float
    reason: str
    severity: int  # 1-5 scale

class HydrologicalQCService:
    """Professional hydrological quality control service"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # WMO-168 compliant QC thresholds
        self.physical_limits = {
            'water_level': {'min': -2.0, 'max': 50.0},  # meters
            'rainfall': {'min': 0.0, 'max': 500.0},     # mm/day
            'temperature': {'min': -50.0, 'max': 60.0}, # Celsius
            'flow': {'min': 0.0, 'max': 100000.0}       # mÂ³/s
        }
        
        # Statistical QC parameters
        self.outlier_threshold = 3.5  # Modified Z-score
        self.spike_threshold = 4.0    # Standard deviations
        self.persistence_threshold = 6  # Hours for stuck values
        self.rate_change_threshold = 5.0  # Max change per hour
        
    def perform_comprehensive_qc(self, data: pd.DataFrame, 
                                 parameter: str = 'depth',
                                 station_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Perform comprehensive quality control following WMO standards
        
        Args:
            data: DataFrame with time_point, depth/value columns
            parameter: Type of hydrological parameter
            station_id: Optional station identifier
        
        Returns:
            Comprehensive QC report with flagged data
        """
        self.logger.info(f"Starting comprehensive QC for {len(data)} records")
        
        # Initialize results
        qc_results = []
        flags = []
        
        # Sort by time (handle different time column names)
        time_column = None
        if 'time_point' in data.columns:
            time_column = 'time_point'
        elif 'Year' in data.columns:
            time_column = 'Year'
        
        if time_column:
            data_sorted = data.sort_values(time_column).copy()
            if time_column == 'Year':
                # For annual data, create fake timestamps
                times = pd.to_datetime(data_sorted['Year'].astype(str) + '-01-01').values
            else:
                times = pd.to_datetime(data_sorted[time_column]).values
        else:
            data_sorted = data.copy()
            # Create fake timestamps if no time column
            times = pd.date_range('2020-01-01', periods=len(data), freq='D').values
        
        values = data_sorted[parameter].values
        
        # 1. Physical Range Checks (WMO Test 1)
        range_flags = self._physical_range_check(values, parameter)
        
        # 2. Gross Error Checks (WMO Test 2) 
        gross_error_flags = self._gross_error_check(values)
        
        # 3. Temporal Consistency Checks (WMO Test 3)
        temporal_flags = self._temporal_consistency_check(values, times)
        
        # 4. Internal Consistency Checks (WMO Test 4)
        internal_flags = self._internal_consistency_check(values)
        
        # 5. Spike Detection (Advanced)
        spike_flags = self._advanced_spike_detection(values)
        
        # 6. Persistence/Stuck Value Detection
        persistence_flags = self._persistence_detection(values, times)
        
        # 7. Rate of Change Checks
        rate_flags = self._rate_of_change_check(values, times)
        
        # 8. Statistical Outlier Detection
        outlier_flags = self._statistical_outlier_detection(values)
        
        # 9. Climatological Checks (if applicable)
        climate_flags = self._climatological_check(values, times, parameter)
        
        # Combine all flags
        combined_flags = self._combine_flags([
            range_flags, gross_error_flags, temporal_flags, internal_flags,
            spike_flags, persistence_flags, rate_flags, outlier_flags, climate_flags
        ])
        
        # Create detailed results
        for i, (value, flag) in enumerate(zip(values, combined_flags)):
            qc_results.append(QCResult(
                flag=flag['status'],
                value=value,
                reason=flag['reason'],
                severity=flag['severity']
            ))
        
        # Generate QC summary
        summary = self._generate_qc_summary(qc_results, values)
        
        # Add flagged data back to dataframe
        data_sorted['qc_flag'] = [r.flag for r in qc_results]
        data_sorted['qc_reason'] = [r.reason for r in qc_results]
        data_sorted['qc_severity'] = [r.severity for r in qc_results]
        
        return {
            'data_with_flags': data_sorted,
            'qc_results': [r.__dict__ for r in qc_results],
            'summary': summary,
            'recommendations': self._generate_recommendations(summary),
            'professional_assessment': self._professional_assessment(summary)
        }
    
    def _physical_range_check(self, values: np.ndarray, parameter: str) -> List[Dict]:
        """WMO Test 1: Physical range checks"""
        flags = []
        limits = self.physical_limits.get(parameter, {'min': -999999, 'max': 999999})
        
        for value in values:
            if pd.isna(value):
                flags.append({'status': 'missing', 'reason': 'Missing value', 'severity': 3})
            elif value < limits['min'] or value > limits['max']:
                flags.append({'status': 'bad', 'reason': f'Outside physical limits ({limits["min"]}-{limits["max"]})', 'severity': 5})
            else:
                flags.append({'status': 'good', 'reason': 'Within physical limits', 'severity': 1})
        
        return flags
    
    def _gross_error_check(self, values: np.ndarray) -> List[Dict]:
        """WMO Test 2: Gross error detection using extreme value analysis"""
        flags = []
        
        # Use robust statistics
        median = np.nanmedian(values)
        mad = np.nanmedian(np.abs(values - median))
        
        # Modified Z-score threshold
        threshold = 3.5
        
        for value in values:
            if pd.isna(value):
                flags.append({'status': 'good', 'reason': 'Already flagged as missing', 'severity': 1})
            else:
                modified_z = 0.6745 * (value - median) / mad if mad > 0 else 0
                if abs(modified_z) > threshold:
                    flags.append({'status': 'suspect', 'reason': f'Extreme value (Modified Z={modified_z:.2f})', 'severity': 3})
                else:
                    flags.append({'status': 'good', 'reason': 'Normal value', 'severity': 1})
        
        return flags
    
    def _temporal_consistency_check(self, values: np.ndarray, times: np.ndarray) -> List[Dict]:
        """WMO Test 3: Temporal consistency checks"""
        flags = []
        
        for i in range(len(values)):
            if i == 0:
                flags.append({'status': 'good', 'reason': 'First observation', 'severity': 1})
                continue
            
            if pd.isna(values[i]) or pd.isna(values[i-1]):
                flags.append({'status': 'good', 'reason': 'Missing adjacent value', 'severity': 1})
                continue
            
            # Calculate time difference in hours
            time_diff = (times[i] - times[i-1]) / np.timedelta64(1, 'h')
            value_diff = abs(values[i] - values[i-1])
            
            # Expected maximum change based on time interval
            max_expected_change = self.rate_change_threshold * time_diff
            
            if value_diff > max_expected_change and time_diff < 24:  # Only for short intervals
                flags.append({'status': 'suspect', 'reason': f'Large temporal change ({value_diff:.2f} in {time_diff:.1f}h)', 'severity': 3})
            else:
                flags.append({'status': 'good', 'reason': 'Temporally consistent', 'severity': 1})
        
        return flags
    
    def _internal_consistency_check(self, values: np.ndarray) -> List[Dict]:
        """WMO Test 4: Internal consistency within measurement"""
        flags = []
        
        # Check for repeated identical values (potential sensor issues)
        for i in range(len(values)):
            if i < 2:
                flags.append({'status': 'good', 'reason': 'Insufficient history', 'severity': 1})
                continue
            
            if pd.isna(values[i]):
                flags.append({'status': 'good', 'reason': 'Missing value', 'severity': 1})
                continue
            
            # Check if last 3 values are identical
            recent_values = values[max(0, i-2):i+1]
            if len(set(recent_values[~pd.isna(recent_values)])) == 1 and len(recent_values) >= 3:
                flags.append({'status': 'suspect', 'reason': 'Repeated identical values', 'severity': 2})
            else:
                flags.append({'status': 'good', 'reason': 'Internally consistent', 'severity': 1})
        
        return flags
    
    def _advanced_spike_detection(self, values: np.ndarray) -> List[Dict]:
        """Advanced spike detection using second derivatives"""
        flags = []
        
        if len(values) < 3:
            return [{'status': 'good', 'reason': 'Insufficient data for spike detection', 'severity': 1}] * len(values)
        
        # Calculate second derivatives
        second_derivatives = np.zeros_like(values)
        for i in range(1, len(values) - 1):
            if not (pd.isna(values[i-1]) or pd.isna(values[i]) or pd.isna(values[i+1])):
                second_derivatives[i] = values[i-1] - 2*values[i] + values[i+1]
        
        # Use robust statistics for threshold
        std_dev = np.nanstd(second_derivatives)
        threshold = self.spike_threshold * std_dev
        
        for i, second_deriv in enumerate(second_derivatives):
            if i == 0 or i == len(values) - 1:
                flags.append({'status': 'good', 'reason': 'Boundary point', 'severity': 1})
            elif pd.isna(values[i]):
                flags.append({'status': 'good', 'reason': 'Missing value', 'severity': 1})
            elif abs(second_deriv) > threshold and threshold > 0:
                flags.append({'status': 'suspect', 'reason': f'Potential spike detected (2nd deriv={second_deriv:.3f})', 'severity': 3})
            else:
                flags.append({'status': 'good', 'reason': 'No spike detected', 'severity': 1})
        
        return flags
    
    def _persistence_detection(self, values: np.ndarray, times: np.ndarray) -> List[Dict]:
        """Detect stuck/persistent values"""
        flags = []
        
        for i in range(len(values)):
            if pd.isna(values[i]):
                flags.append({'status': 'good', 'reason': 'Missing value', 'severity': 1})
                continue
            
            # Look back to find consecutive identical values
            persistence_count = 1
            j = i - 1
            while j >= 0 and not pd.isna(values[j]) and values[j] == values[i]:
                persistence_count += 1
                j -= 1
            
            # Calculate persistence duration in hours
            if i > 0 and persistence_count > 1:
                duration_hours = (times[i] - times[max(0, i - persistence_count + 1)]) / np.timedelta64(1, 'h')
                
                if duration_hours > self.persistence_threshold:
                    flags.append({'status': 'suspect', 'reason': f'Persistent value for {duration_hours:.1f} hours', 'severity': 2})
                else:
                    flags.append({'status': 'good', 'reason': 'Normal variation', 'severity': 1})
            else:
                flags.append({'status': 'good', 'reason': 'Normal variation', 'severity': 1})
        
        return flags
    
    def _rate_of_change_check(self, values: np.ndarray, times: np.ndarray) -> List[Dict]:
        """Check rate of change between consecutive measurements"""
        flags = []
        
        for i in range(len(values)):
            if i == 0:
                flags.append({'status': 'good', 'reason': 'First observation', 'severity': 1})
                continue
            
            if pd.isna(values[i]) or pd.isna(values[i-1]):
                flags.append({'status': 'good', 'reason': 'Missing adjacent value', 'severity': 1})
                continue
            
            # Calculate rate of change
            time_diff_hours = (times[i] - times[i-1]) / np.timedelta64(1, 'h')
            value_change = abs(values[i] - values[i-1])
            
            if time_diff_hours > 0:
                rate = value_change / time_diff_hours
                
                if rate > self.rate_change_threshold:
                    flags.append({'status': 'suspect', 'reason': f'High rate of change ({rate:.2f}/hour)', 'severity': 3})
                else:
                    flags.append({'status': 'good', 'reason': 'Normal rate of change', 'severity': 1})
            else:
                flags.append({'status': 'suspect', 'reason': 'Zero time interval', 'severity': 2})
        
        return flags
    
    def _statistical_outlier_detection(self, values: np.ndarray) -> List[Dict]:
        """Advanced statistical outlier detection"""
        flags = []
        
        # Use multiple methods for robustness
        valid_values = values[~pd.isna(values)]
        if len(valid_values) < 5:
            return [{'status': 'good', 'reason': 'Insufficient data for statistical analysis', 'severity': 1}] * len(values)
        
        # Method 1: Modified Z-score
        median = np.median(valid_values)
        mad = np.median(np.abs(valid_values - median))
        
        # Method 2: IQR method
        Q1, Q3 = np.percentile(valid_values, [25, 75])
        IQR = Q3 - Q1
        iqr_lower = Q1 - 1.5 * IQR
        iqr_upper = Q3 + 1.5 * IQR
        
        for value in values:
            if pd.isna(value):
                flags.append({'status': 'good', 'reason': 'Missing value', 'severity': 1})
                continue
            
            # Modified Z-score
            if mad > 0:
                modified_z = 0.6745 * (value - median) / mad
                is_outlier_z = abs(modified_z) > self.outlier_threshold
            else:
                is_outlier_z = False
            
            # IQR method
            is_outlier_iqr = value < iqr_lower or value > iqr_upper
            
            if is_outlier_z and is_outlier_iqr:
                flags.append({'status': 'suspect', 'reason': 'Statistical outlier (multiple methods)', 'severity': 4})
            elif is_outlier_z or is_outlier_iqr:
                flags.append({'status': 'suspect', 'reason': 'Potential statistical outlier', 'severity': 2})
            else:
                flags.append({'status': 'good', 'reason': 'Statistically normal', 'severity': 1})
        
        return flags
    
    def _climatological_check(self, values: np.ndarray, times: np.ndarray, parameter: str) -> List[Dict]:
        """Climatological consistency checks"""
        flags = []
        
        # Simple seasonal expectation model (can be enhanced with historical data)
        for i, (value, time) in enumerate(zip(values, times)):
            if pd.isna(value):
                flags.append({'status': 'good', 'reason': 'Missing value', 'severity': 1})
                continue
            
            # Extract month for seasonal analysis
            dt = pd.to_datetime(time)
            month = dt.month
            
            # Simple seasonal thresholds (should be calibrated with local data)
            seasonal_factor = 1.0
            if parameter == 'water_level':
                # Higher levels expected in wet season (example: May-October)
                if 5 <= month <= 10:
                    seasonal_factor = 1.5
                else:
                    seasonal_factor = 0.8
            
            # This is a simplified check - in practice, use historical percentiles
            median_val = np.nanmedian(values)
            expected_max = median_val * seasonal_factor * 3
            
            if value > expected_max:
                flags.append({'status': 'suspect', 'reason': f'Exceeds seasonal expectations for month {month}', 'severity': 2})
            else:
                flags.append({'status': 'good', 'reason': 'Climatologically reasonable', 'severity': 1})
        
        return flags
    
    def _combine_flags(self, flag_lists: List[List[Dict]]) -> List[Dict]:
        """Combine multiple QC test results using worst-case approach"""
        if not flag_lists or not flag_lists[0]:
            return []
        
        n_records = len(flag_lists[0])
        combined_flags = []
        
        for i in range(n_records):
            # Get all flags for this record
            record_flags = [flags[i] for flags in flag_lists if i < len(flags)]
            
            # Determine worst status
            statuses = [f['status'] for f in record_flags]
            severities = [f['severity'] for f in record_flags]
            reasons = [f['reason'] for f in record_flags if f['status'] != 'good']
            
            # Priority: bad > suspect > good > missing
            if 'bad' in statuses:
                final_status = 'bad'
            elif 'suspect' in statuses:
                final_status = 'suspect'
            elif 'missing' in statuses:
                final_status = 'missing'
            else:
                final_status = 'good'
            
            combined_flags.append({
                'status': final_status,
                'severity': max(severities) if severities else 1,
                'reason': '; '.join(reasons) if reasons else 'Passes all QC tests'
            })
        
        return combined_flags
    
    def _generate_qc_summary(self, qc_results: List[QCResult], values: np.ndarray) -> Dict[str, Any]:
        """Generate comprehensive QC summary"""
        total_records = len(qc_results)
        
        # Count flags
        flag_counts = {}
        severity_counts = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
        
        for result in qc_results:
            flag_counts[result.flag] = flag_counts.get(result.flag, 0) + 1
            severity_counts[result.severity] = severity_counts.get(result.severity, 0) + 1
        
        # Calculate percentages
        flag_percentages = {flag: (count / total_records) * 100 for flag, count in flag_counts.items()}
        
        # Data completeness
        missing_count = flag_counts.get('missing', 0)
        completeness = ((total_records - missing_count) / total_records) * 100
        
        # Quality score (0-100)
        quality_score = self._calculate_overall_quality_score(flag_counts, total_records)
        
        return {
            'total_records': total_records,
            'flag_counts': flag_counts,
            'flag_percentages': flag_percentages,
            'severity_distribution': severity_counts,
            'data_completeness': completeness,
            'quality_score': quality_score,
            'suspect_and_bad_percentage': flag_percentages.get('suspect', 0) + flag_percentages.get('bad', 0),
            'professional_grade': quality_score >= 85 and flag_percentages.get('bad', 0) < 1.0
        }
    
    def _calculate_overall_quality_score(self, flag_counts: Dict[str, int], total: int) -> float:
        """Calculate overall quality score (0-100) for professional use"""
        score = 100.0
        
        # Penalties for different flag types
        score -= (flag_counts.get('bad', 0) / total) * 50      # Heavy penalty for bad data
        score -= (flag_counts.get('suspect', 0) / total) * 25  # Moderate penalty for suspect data
        score -= (flag_counts.get('missing', 0) / total) * 10  # Light penalty for missing data
        
        return max(0, score)
    
    def _generate_recommendations(self, summary: Dict[str, Any]) -> List[str]:
        """Generate professional recommendations based on QC results"""
        recommendations = []
        
        bad_pct = summary['flag_percentages'].get('bad', 0)
        suspect_pct = summary['flag_percentages'].get('suspect', 0)
        missing_pct = summary['flag_percentages'].get('missing', 0)
        
        if bad_pct > 5:
            recommendations.append("HIGH PRIORITY: >5% bad data detected - investigate sensor calibration and maintenance")
        elif bad_pct > 1:
            recommendations.append("MEDIUM PRIORITY: >1% bad data - review measurement procedures")
        
        if suspect_pct > 10:
            recommendations.append("Review suspect data flags - may indicate systematic measurement issues")
        
        if missing_pct > 10:
            recommendations.append("High data loss rate - check data transmission and logging systems")
        
        if summary['data_completeness'] < 90:
            recommendations.append("Data completeness below 90% - not suitable for critical frequency analysis")
        
        if summary['quality_score'] < 85:
            recommendations.append("Quality score below professional standards - implement enhanced QC procedures")
        
        if not recommendations:
            recommendations.append("Data quality meets professional hydrological standards")
        
        return recommendations
    
    def _professional_assessment(self, summary: Dict[str, Any]) -> Dict[str, Any]:
        """Professional assessment following international standards"""
        
        # WMO-168 compliance assessment
        wmo_compliant = (
            summary['flag_percentages'].get('bad', 0) < 2.0 and
            summary['data_completeness'] > 90 and
            summary['quality_score'] > 80
        )
        
        # Frequency analysis suitability
        frequency_suitable = (
            summary['flag_percentages'].get('bad', 0) < 1.0 and
            summary['data_completeness'] > 95 and
            summary['quality_score'] > 85
        )
        
        # Professional certification level
        if summary['quality_score'] >= 95 and summary['flag_percentages'].get('bad', 0) < 0.5:
            cert_level = "EXCELLENT - Suitable for critical infrastructure design"
        elif summary['quality_score'] >= 85 and summary['flag_percentages'].get('bad', 0) < 1.0:
            cert_level = "GOOD - Suitable for frequency analysis and water management"
        elif summary['quality_score'] >= 75:
            cert_level = "ACCEPTABLE - Requires enhanced QC procedures"
        else:
            cert_level = "INADEQUATE - Not suitable for professional use"
        
        return {
            'wmo_168_compliant': wmo_compliant,
            'frequency_analysis_suitable': frequency_suitable,
            'professional_certification': cert_level,
            'overall_grade': 'A' if summary['quality_score'] >= 90 else 
                           'B' if summary['quality_score'] >= 80 else
                           'C' if summary['quality_score'] >= 70 else 'F',
            'standards_compliance': {
                'WMO-168': wmo_compliant,
                'ISO_14688': frequency_suitable,
                'ASCE_Standards': summary['professional_grade']
            }
        }